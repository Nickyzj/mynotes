### 自我介绍

- 从事软件测试工作有10年以上经验
- 主要负责自动化测试和性能测试和接口测试等
- 在过去的工作经历中，曾带领测试组参与多个大型项目的测试工作
- 对测试流程，测试管理，测试方法等都有较深的实践和体会
- 有基于 QC/UFT 和 TestNG/Selenium 开源框架开发经验
- 开发测试服务器，测试工具经验，开发监控系统
- 编程语言熟悉 python，java
- 熟悉 Linux，mysql



### 如何写测试计划，包含哪些部分

* 测试目标。根据商业目标，质量目标，制定测试计划

  * 质量目标 - 依据产品商业价值，对公司业务重要度不同，测试目标不同
    * 对业务重要的系统要求质量第一（0 bug 上线）。要求线上事故率，无 p0和p1级事故。
    * 对不那么重要的系统，快速上线
    * 项目铁三角，时间，成本，质量
  * 需求分析。关注测试优先级，测试重点，难点，关键路径
    * 以思维导图形式，初步梳理模块，每个模块内功能。
    * 根据业务场景梳理功能关键路径
    * 按功能优先级划分（主要功能，次要功能）
    * 功能于功能之间强关联功能，弱关联功能

  * 测试策略

    * 整体测试策略
      * 多样性测试：业务场景，交互，UI，兼容性，数据逻辑，业务逻辑，性能测试，尽可能发觉问题。
      * 组内组外充分沟通，发掘关注盲区
    * 版本测试策略，测试颗粒度
      * P1重点功能，充分分析业务场景，测试用例详细设计，然后进行探索式测试
      * P2主要功能，测试用例详细设计，不需要探索式测试
      * P3次要功能，仅编写测试要点，进行探索式测试
      * P4一般功能，根据需求文档进行测试

  * 制定计划

    * 在测试之前安排测试用例编写，测试用例评审
    * 工期，人员资源安排 
      * 根据模块，任务量人天分配任务到每个人，设置里程碑
    * 分阶段测试：
      * 开发接口提测时间
      * 分模块提测时间
      * 开发整体提测时间
    * 风险处理
      * 预先识别的风险，指定风险策略
      * 资源，时间，成本估算时要留有余地
      * 关键性岗位要有后备人员
      * 文档完整，及时，便于知识分享和移交
        * 测试日志，版本控制文档，整体技术文档（测试策略，测试用例）
      * 相互评审，不同人员交叉测试
      * 跟踪工作过程，及时发现风险迹象。

    **1、需求风险**

    产品需求的不明确，对产品需求理解不准确，导致测试范围存在误差，遗漏部分需求或者执行了错误的测试方式；另外需求变更导致测试用例变更，测试用例维护成本增加，实时更新时存在误差。

    **2、测试用例风险**

    测试用例设计不完整，忽视了边界条件、异常输入等情况，用例覆盖率没有做到足够覆盖，测试用例没有得到全部执行，有些用例被有意或者无意的漏测，需求变更导致的测试时间被压缩等情况。

    **3、缺陷风险**

    某些缺陷偶发，难以重现，容易被遗漏；缺陷跟踪不够积极主动，没做好缺陷记录和及时更新，同样的缺陷，导致的原因可能不同，对这点没意识到导致的线上生产问题等。

    **4、代码质量风险**

    代码质量差，可读性差，重构性差，没做好注释等原因导致缺陷较多，修改难度增大；另外还有系统架构设计的不足，导致的扩展性不足，性能兼容差等问题。

    **5、测试环境风险**

    测试环境和生产环境配置不同，测试环境交叉影响较大，测试环境数据量不足导致的测试结果误差等问题。

    **6、测试技术风险**

    某些项目存在技术难度，测试能力和经验所限，技术水平相对较差导致测试进展缓慢，测试结果准确性不够，项目发布日期延期等问题。

    **7、回归测试风险**

    回归测试，一般时间相对来说较少，且大多只回归主要的功能点用例，可能造成漏测；另外还有回归验证缺陷时业务流走不通导致的打回修复再验证造成的时间延后问题。

    **8、沟通协调风险**

    项目进行过程中需要多方沟通协调，不同部门，岗位之间的沟通、协作，难免存在误解、沟通不畅的情况，比如需求变更没有及时沟通，开发代码提交没有及时告知，测试结果的反馈不及时等问题。

    **9、研发流程风险**

    其中包括从产品需求评审、研发设计、代码提交、测试发布等一些列流程，流程的不规范不协调很可能导致很多问题；比如开发在不告知其他成员的情况下提交代码，发布没有预生产环境，生产出现

    问题无法及时回滚等很多说烂了的情况。流程没必要一板一眼的执行，但没有流程是万万不行的。

其中有的风险是难以避免的，如缺陷风险；有的风险从理论上可以避免，如需求风险，沟通风险等；还有些风险在实际操作过程中出于时间和成本的考虑，也难以完全回避，如回归测试风险等。

对于这些风险的存在，要尽量避免，也要做好备份方案和容灾机制，规范流程，明确职责，尽可能将风险降到可接受范围内



### 如何做接口测试

* 接口文档：地址，参数类型，方法，输入，输出
* 识别接口类型，对内接口，对外接口
* web service （xml），http （json）
* 测试系统安全性，稳定性，性能。接口会绕过 UI 限制，传入非法内容。
  * 业务功能（正常，异常场景）
  * 业务规则（覆盖度）
  * 参数验证（边界）
  * 异常场景（重复提交，并发提交，事务中断，大数据量）
  * 性能测试（响应时间，吞吐量，并发数，资源要求）
  * 安全测试（权限验证，SQL 注入）
* 检查重点
  * 模拟客户端发送请求报文，服务器接收报文后做处理并返回应答。
    * 基本测试：针对基本业务功能测试，返回数据与预期是否一致
  * 检查数据交互，传递过程，处理次数
  * 接口容错性
  * 参数边界值（+1，-1，null，超长）
    * 边界分析测试：跳过 UI 限制，可覆盖范围广。出现问题概率高
  * 接口性能
    * 性能测试：响应时间，并发，服务端资源使用。需另外单独做。
  * 安全性，尤其对外部调用
* 自动化持续集成，相对于 UI 自动化更稳定。减少回归测试的人力成本和时间
  * 工具：postman，jmeter，java+httpclient，python+request

### 如何做质量管理



### 开发，测试，冲突管理

一、问题确认与评估

再次论证该问题确实是程序缺陷，并评估该缺陷的重要程度并对其分类。比如可存在以下分类：

1、设计文档范围内的功能性缺陷

2、影响到程序的安全性和稳定性缺陷

3、界面缺陷

4、一般性错误（如未考虑边界检查等）

5、边缘死角，规律不明显，不太容易重现的错误

6、兼容性错误（例如旧机型、CPU\MEM,旧标准等等）

7、安全性或易用性等的修改建议

二、明确Dev不修改该缺陷的确切原因

比如可存在以下原因：

1、规律不明显，不好重现

2、dev认为是不影响主要功能的一般性bug,因时间处于版本的稳定期，担心牵一发动全身引起更多错误

3、调用了第三方组件或库函，是第三方程序存在的缺陷

4、存在技术难点

5、设计本身存在问题，程序逻辑是正确的，但实现结果并非用户所需（换言之，dev说这是设计问题，不是程序问题）

6、Dev的个人主观意见：

·该瑕疵可以容忍，没必要修改

·修改该瑕疵会引起更大的问题

7、Tester和dev对错误的理解有分歧：

·tester理解错误，该问题并不是bug

·tester没有说服dev这是个bug

三、具体问题具体分析

分析完第一、二步之后，也就基本上明确了问题的争议焦点，然后具体问题具体分析。

1、如果dev认为不好重现，则tester有责任和义务找到更简洁有效的重现规律。

2、如果tester没有说服dev认识到这是个缺陷，则需要拿出强有力的证据（测试用例、设计文档、错误现象等）来证明。

3、对于第三方库函bug,或技术难点导致的bug,则坚持原则--宁缺勿滥，必要时宁可封掉该功能。

4、针对错误的设计、不能说服的dev主观理解、改动隐患，以及稳定期等特殊情况，则可通过TM进行多方沟通。

四、发挥TM与PM的沟通职责

强调沟通

TM和PM有团队沟通的职责。在bug分类、指派和反馈过程中出现有争议的问题时，TM和PM有责任和义务进行干预。根据问题的重要程度和轻重缓急，采取不同的方式进行沟通。如出现“三”中3、4类较大争议的问题，可通过会议研讨等形式召集多方进行论证，并达成一致的解决意见，解决方法形成备忘录。

对因各种原因继续保留在发布版本中的bug,尤其可能影响功能的，应予以说明，提醒用户绕过。



团队管理

jenkins 主从配置

为什么喜欢做测试

为什么喜欢做自动化测试

为什么喜欢做性能测试

有挑战的项目，任务

jvm 架构，如何监控

Linux 如何监控

排序，时间复杂度

读文件，写文件

介绍自动化测试，

介绍性能测试，如何监控

测试中发现什么问题，如何解决

Springboot 优缺点