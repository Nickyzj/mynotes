https://www.cnblogs.com/imyalost/

https://www.cnblogs.com/imyalost/p/10012867.html

### 容量测试：

要计算一个系统需要多少台机器，除了需要知道未来的业务调用量之外，还有一个更重要的变量，就是单台机器的服务能力。

两种方法：

* 模拟请求：通过对生产环境的一台机器发起模拟请求调用来达到压力测试的目的

* 请求转发：将分布式环境中多台机器的请求转发到一台机器上

### 全链路压测：

现状：业务逻辑太复杂、系统庞大、子系统较多、系统间解耦程度较低、调用链路较长、核心系统环环相扣。

1. 只能进行**独立系统**的压测工作，导致压测任务量较大；

2. 强依赖系统较多，第三方调用面临种种限制，只能通过mock方式解决；

3. 没有较为独立的性能测试环境，UAT和PAT测试数据差异较大，无法给予上线一个较为准确的容量评估；

4. 项目排期没有预留足够的性能测试时间，导致需要经常加班甚至通宵；

**面临的挑战**

技术问题之外，要开展全链路压测，还面临如下的几点挑战：

1. 由于全链路压测涉及的系统及场景较多，因此需要**跨团队沟通、跨系统协调改造**，公司体量越大，这一点难度就越大；

2. 全链路压测涉及的系统较多，且不同的系统架构也有所不同，因此需要考虑：**机房管理、基础网络、DB管理、持久存储、中间件、应用部署、流量接入、监控与运维保障等多方面**；

3. 全链路压测的目的是**找到系统调用链路薄弱环节并优化**，这就要求对整个调用链路涉及的系统进行进行准确的容量规划，因此**环境和配置**，是必须重视的一点；

不过全链路压测的优点也很明显，比如：优化联络薄弱环节可以**提高系统的可用性**，容量规划可以**节省成本，提高效率**。

**开展前的准备工作**

在开展全链路压测之前，我们需要做哪些准备工作？

1. 业务梳理：覆盖全部的业务场景，是难度很大且不理智的选择，一般来说只需要筛选出**高频使用的功能、核心功能以及基础功能**即可；

2. 场景梳理：场景梳理也是很重要的一项工作，因为只有确定了被测场景，我们才能设计合理的测试方案和策略，场景覆盖正常操作、异常操作即可；

3. 流量模型：需要通过监控分析等手段，得到日常流量场景、峰值流量场景下各系统的流量以及配比，进行一定的放大，来作为全链路压测的流量参考模型；

4. 数据处理：全链路压测通常在生产环境进行，所以防止数据污染是必须考虑的问题，一般来说都是通过对入口流量进行标记区分、数据隔离、影子库等方式来避免，当然，还需要做好灾备工作；

5. 实时监控：无论是压测开始前还是测试进行中，都需要及时且可视化的获取到系统的状态变化，方便及时排查定位问题，也避免压测对正常的服务造成干扰；

　　监控的重点，主要是对应服务的TPS、不同百分比的RT、成功率、资源耗用、服务状态、告警等信息；

**全链路压测平台架构设计**

全链路压测平台的架构设计，主要由以下几部分组成：

1. Controller：主要任务为压测任务分配、Agent管理；Performance Center 或 Jmeter

②、Load generator：负责压测任务下发，执行脚本

⑤、实时监控：Sitescope 或 Jmeter + influxDB，并结合Grafana等可视化工具进行界面展示；

⑥、Log Service：日志服务，即无论压测机还是服务应用在测试过程中产生的日志，都统一收集，方便进行问题排查定位；

⑦、Elasticsearch/Influxdb：对压测产生的数据存储；

⑧、Git：压测脚本的版本管理；

⑨、Gitlab：作为数据仓库进行版本管理，Agent主动拉取脚本执行；

⑩、配置信息管理：MongoDB 或 Redis

模拟的场景要尽可能贴近真实场景

测试数据：提取同等数量级的基础数据（主数据）

雪崩效应：当一台机器超负荷运转的时候，这台处理请求的时间会变长。这会给用户带来不好的体验，用户会试图重复提交请求，这无形中又给系统带来了更多的请求压力。随着请求堆积的越来越多，系统性能会逐渐下降甚至无法响应新的请求。

当一台机器挂掉以后, 负载均衡会把请求重定向到另外的机器上去，这又无形中给别的机器带来了更多的任务，而这些机器也处于一个饱和的状态，很快也会像第一台机器一样，也无法响应新的请求。就这样，在很短的时间之内，越来越多的机器会停止响应，最终导致整个集群都无法响应。这就使我们常常说的“雪崩效应”。